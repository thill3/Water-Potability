{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('water_potability.csv') #This works becuase the \n",
    "    #Jupyter Notebook is already in the folder where the data file lives.\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3276, 10)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2011/3276"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#60 percent of the rows have no nulls.\n",
    "#So just dropping everything seems like a bad idea.\n",
    "#So create a pipeline to test imputation methods\n",
    "    #[split into train and test]\n",
    "    #impute for each column that needs it - ph, Sulfate, Trihalomethanes\n",
    "    #normalize\n",
    "    #run through regressions\n",
    "        #logistic\n",
    "        #SVM\n",
    "        #KNN\n",
    "        #RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the train data into train and val\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, val_set = train_test_split(data, test_size=.4, random_state=42)\n",
    "#print(\"train shape = \", train_set.shape)\n",
    "#print(\"val shape = \", val_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_set, test_set = train_test_split(val_set, test_size = .5, random_state = 42)\n",
    "#print(\"train shape = \", train_set.shape)\n",
    "#print(\"val shape = \", val_set.shape)\n",
    "#print(\"valid shape = \", valid_set.shape)\n",
    "#print(\"test shape = \", test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_values = train_set.pop('Potability') #remove the column from the df and save it elsewhere\n",
    "valid_set_values = valid_set.pop('Potability') #remove the column from the df and save it elsewhere\n",
    "test_set_values = test_set.pop('Potability') #remove the column from the df and save it elsewhere\n",
    "\n",
    "#print(\"train shape = \", train_set.shape)\n",
    "#print(\"val shape = \", val_set.shape)\n",
    "#print(\"valid shape = \", valid_set.shape)\n",
    "#print(\"test shape = \", test_set.shape)\n",
    "\n",
    "#print(\"train values shape = \", train_set_values.shape)\n",
    "#print(\"valid values shape = \", valid_set_values.shape)\n",
    "#print(\"test values shape = \", test_set_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1965+655+656"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import StandardScaler\n",
    "#scale = StandardScaler()\n",
    "#scaled = scale.fit_transform(train_set)\n",
    "#print(scaled)\n",
    "#So it will work to just throw the dataset at it. That's what I wanted to know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  14 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=6)]: Done  79 out of  90 | elapsed:    0.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'imputer__strategy': 'mean', 'regressor__C': 10000000000, 'regressor__penalty': 'l2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done  90 out of  90 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "#logistic pipe\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "full_pipe_log = Pipeline([\n",
    "    ('imputer', SimpleImputer()), #there will, ultimately, be three options here.\n",
    "        #mean, median, and node\n",
    "    ('scaler', StandardScaler()), #this is statistical normalization (z - mu)/s.d.\n",
    "    ('regressor', LogisticRegression())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "        'imputer__strategy': [\"most_frequent\",\"mean\", \"median\"],\n",
    "        'regressor__C': [10**10,10**9,10**8],\n",
    "        'regressor__penalty': ['l1', 'l2']\n",
    "        }\n",
    "\n",
    "method_search = GridSearchCV(\n",
    "    full_pipe_log\n",
    "    , param_grid\n",
    "    , cv = 5\n",
    "    , scoring = 'neg_mean_squared_error'\n",
    "    , verbose = 4\n",
    "    , n_jobs = 6)\n",
    "method_search.fit(train_set, train_set_values)\n",
    "print(method_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[[419 236]]\n",
      "0.6396946564885496\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "pred_values = method_search.predict(valid_set)\n",
    "ct = np.array(pd.crosstab(pred_values, valid_set_values))\n",
    "print(pred_values)\n",
    "print(ct)\n",
    "print(ct.trace()/ct.sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.639 is what I had without messing with hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  14 tasks      | elapsed:    0.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'imputer__strategy': 'mean', 'regressor__C': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done  45 out of  45 | elapsed:    1.3s finished\n"
     ]
    }
   ],
   "source": [
    "#SVM pipe\n",
    "from sklearn.svm import SVC\n",
    "full_pipe_SV = Pipeline([\n",
    "    ('imputer', SimpleImputer()), #there will, ultimately, be three options here.\n",
    "        #mean, median, and node\n",
    "    ('scaler', StandardScaler()), #this is statistical normalization (z - mu)/s.d.\n",
    "    ('regressor', SVC()) #will need to specify kernel type (linear, polynomial, RBF)\n",
    "])\n",
    "#Also specify regularization term C for how much error is OK\n",
    "#Also specify gamma for tightness of fit\n",
    "param_grid = {\n",
    "        'imputer__strategy': [\"most_frequent\",\"mean\", \"median\"],\n",
    "        'regressor__C': [10**1,10**0,10**-1]\n",
    "        }\n",
    "\n",
    "method_search = GridSearchCV(\n",
    "    full_pipe_SV\n",
    "    , param_grid\n",
    "    , cv = 5\n",
    "    , scoring = 'neg_mean_squared_error'\n",
    "    , verbose = 4\n",
    "    , n_jobs = 6)\n",
    "method_search.fit(train_set, train_set_values)\n",
    "print(method_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1\n",
      " 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1\n",
      " 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0\n",
      " 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0]\n",
      "[[384 164]\n",
      " [ 35  72]]\n",
      "0.6961832061068702\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "pred_values = method_search.predict(valid_set)\n",
    "ct = np.array(pd.crosstab(pred_values, valid_set_values))\n",
    "print(pred_values)\n",
    "print(ct)\n",
    "print(ct.trace()/ct.sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.696 is better than I had"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  14 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=6)]: Done 160 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=6)]: Done 584 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=6)]: Done 1268 tasks      | elapsed:   13.5s\n",
      "[Parallel(n_jobs=6)]: Done 2152 tasks      | elapsed:   23.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'imputer__strategy': 'median', 'regressor__algorithm': 'ball_tree', 'regressor__leaf_size': 1, 'regressor__n_neighbors': 4, 'regressor__p': 2, 'regressor__weights': 'uniform'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done 2700 out of 2700 | elapsed:   29.3s finished\n"
     ]
    }
   ],
   "source": [
    "#KNN pipe\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "full_pipe_KNN = Pipeline([\n",
    "    ('imputer', SimpleImputer()), #there will, ultimately, be three options here.\n",
    "        #mean, median, and node\n",
    "    ('scaler', StandardScaler()), #this is statistical normalization (z - mu)/s.d.\n",
    "    ('regressor', KNeighborsClassifier())  #need to specify number of neighbors. Do this in the grid search\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "        'imputer__strategy': [\"most_frequent\",\"mean\", \"median\"],\n",
    "        'regressor__n_neighbors': [3,4,5],\n",
    "        'regressor__algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "        'regressor__weights':['uniform','distance'],\n",
    "        'regressor__leaf_size':[1,2,3,4,5],\n",
    "        'regressor__p':[1,2],\n",
    "        }\n",
    "\n",
    "method_search = GridSearchCV(\n",
    "    full_pipe_KNN\n",
    "    , param_grid\n",
    "    , cv = 5\n",
    "    , scoring = 'neg_mean_squared_error'\n",
    "    , verbose = 4\n",
    "    , n_jobs = 6)\n",
    "method_search.fit(train_set, train_set_values)\n",
    "print(method_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1\n",
      " 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0\n",
      " 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0\n",
      " 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1\n",
      " 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0\n",
      " 0 0 0 0 1 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      "[[373 175]\n",
      " [ 46  61]]\n",
      "0.6625954198473283\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "pred_values = method_search.predict(valid_set)\n",
    "ct = np.array(pd.crosstab(pred_values, valid_set_values))\n",
    "print(pred_values)\n",
    "print(ct)\n",
    "print(ct.trace()/ct.sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.662 is a bit better, still not very good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  13 tasks      | elapsed:   21.1s\n",
      "[Parallel(n_jobs=6)]: Done  86 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=6)]: Done 180 out of 180 | elapsed:  4.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'imputer__strategy': 'median', 'regressor__criterion': 'gini', 'regressor__min_samples_leaf': 1, 'regressor__n_estimators': 800}\n"
     ]
    }
   ],
   "source": [
    "#RF pipe\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "full_pipe_RF = Pipeline([\n",
    "    ('imputer', SimpleImputer()), #there will, ultimately, be three options here.\n",
    "        #mean, median, and node\n",
    "    ('scaler', StandardScaler()), #this is statistical normalization (z - mu)/s.d.\n",
    "    ('regressor', RandomForestClassifier())  \n",
    "])\n",
    "#max depth\n",
    "\n",
    "param_grid = {\n",
    "        'imputer__strategy': [\"most_frequent\",\"mean\", \"median\"],\n",
    "        'regressor__n_estimators': [800,1000,1200],\n",
    "        'regressor__min_samples_leaf':[1,2],\n",
    "        'regressor__criterion': ['gini', 'entropy']\n",
    "        }\n",
    "\n",
    "\n",
    "method_search = GridSearchCV(\n",
    "    full_pipe_RF\n",
    "    , param_grid\n",
    "    , cv = 5\n",
    "    , scoring = 'neg_mean_squared_error'\n",
    "    , verbose = 4\n",
    "    , n_jobs = 6)\n",
    "method_search.fit(train_set, train_set_values)\n",
    "print(method_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
      " 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0\n",
      " 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0\n",
      " 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      "[[376 158]\n",
      " [ 43  78]]\n",
      "0.6931297709923664\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "pred_values = method_search.predict(valid_set)\n",
    "ct = np.array(pd.crosstab(pred_values, valid_set_values))\n",
    "print(pred_values)\n",
    "print(ct)\n",
    "print(ct.trace()/ct.sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.688 is little improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  13 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=6)]: Done  86 tasks      | elapsed:   19.8s\n",
      "[Parallel(n_jobs=6)]: Done 209 tasks      | elapsed:   49.8s\n",
      "[Parallel(n_jobs=6)]: Done 380 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=6)]: Done 405 out of 405 | elapsed:  1.7min finished\n",
      "c:\\users\\t3\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:15:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "{'imputer__strategy': 'mean', 'regressor__learning_rate': 0.05, 'regressor__max_depth': 35, 'regressor__n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "#XGBoost pipe\n",
    "from xgboost import XGBClassifier\n",
    "full_pipe_XGB = Pipeline([\n",
    "    ('imputer', SimpleImputer()), #there will, ultimately, be three options here.\n",
    "        #mean, median, and node\n",
    "    ('scaler', StandardScaler()), #this is statistical normalization (z - mu)/s.d.\n",
    "    ('regressor', XGBClassifier())  \n",
    "])\n",
    "#max depth\n",
    "\n",
    "param_grid = {\n",
    "        'imputer__strategy': [\"most_frequent\",\"mean\", \"median\"],\n",
    "        'regressor__learning_rate': [.07,.05,.03],\n",
    "        'regressor__max_depth': [30,35,40],\n",
    "        'regressor__n_estimators': [40,50,60]\n",
    "        }\n",
    "\n",
    "\n",
    "method_search = GridSearchCV(\n",
    "    full_pipe_XGB\n",
    "    , param_grid\n",
    "    , cv = 5\n",
    "    , scoring = 'neg_mean_squared_error'\n",
    "    , verbose = 4\n",
    "    , n_jobs = 6)\n",
    "method_search.fit(train_set, train_set_values)\n",
    "print(method_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1\n",
      " 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0\n",
      " 0 1 0 1 1 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
      " 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1\n",
      " 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0\n",
      " 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0\n",
      " 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 1 1 0\n",
      " 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 1 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
      " 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1\n",
      " 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0\n",
      " 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0]\n",
      "[[348 135]\n",
      " [ 71 101]]\n",
      "0.6854961832061068\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "pred_values = method_search.predict(valid_set)\n",
    "ct = np.array(pd.crosstab(pred_values, valid_set_values))\n",
    "print(pred_values)\n",
    "print(ct)\n",
    "print(ct.trace()/ct.sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.685 on validation set with XGBoost? There has to be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I guess the support vector classifier is the best I'm going to get.\n",
    "#Others on Kaggle have managed 85 percent though.\n",
    "#May be that I'm overcomplicating things."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
