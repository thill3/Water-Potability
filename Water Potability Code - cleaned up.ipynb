{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Preprocessing packages\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Classification algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('water_potability.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3276 entries, 0 to 3275\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   ph               2785 non-null   float64\n",
      " 1   Hardness         3276 non-null   float64\n",
      " 2   Solids           3276 non-null   float64\n",
      " 3   Chloramines      3276 non-null   float64\n",
      " 4   Sulfate          2495 non-null   float64\n",
      " 5   Conductivity     3276 non-null   float64\n",
      " 6   Organic_carbon   3276 non-null   float64\n",
      " 7   Trihalomethanes  3114 non-null   float64\n",
      " 8   Turbidity        3276 non-null   float64\n",
      " 9   Potability       3276 non-null   int64  \n",
      "dtypes: float64(9), int64(1)\n",
      "memory usage: 256.1 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So we know that there are 10 columns. All are numerical.\n",
    "#The last one is Potability, our target value (1 or 0)\n",
    "#We see that three of the columns are missing values. We will have to deal with that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ph</th>\n",
       "      <th>Hardness</th>\n",
       "      <th>Solids</th>\n",
       "      <th>Chloramines</th>\n",
       "      <th>Sulfate</th>\n",
       "      <th>Conductivity</th>\n",
       "      <th>Organic_carbon</th>\n",
       "      <th>Trihalomethanes</th>\n",
       "      <th>Turbidity</th>\n",
       "      <th>Potability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2785.000000</td>\n",
       "      <td>3276.000000</td>\n",
       "      <td>3276.000000</td>\n",
       "      <td>3276.000000</td>\n",
       "      <td>2495.000000</td>\n",
       "      <td>3276.000000</td>\n",
       "      <td>3276.000000</td>\n",
       "      <td>3114.000000</td>\n",
       "      <td>3276.000000</td>\n",
       "      <td>3276.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.080795</td>\n",
       "      <td>196.369496</td>\n",
       "      <td>22014.092526</td>\n",
       "      <td>7.122277</td>\n",
       "      <td>333.775777</td>\n",
       "      <td>426.205111</td>\n",
       "      <td>14.284970</td>\n",
       "      <td>66.396293</td>\n",
       "      <td>3.966786</td>\n",
       "      <td>0.390110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.594320</td>\n",
       "      <td>32.879761</td>\n",
       "      <td>8768.570828</td>\n",
       "      <td>1.583085</td>\n",
       "      <td>41.416840</td>\n",
       "      <td>80.824064</td>\n",
       "      <td>3.308162</td>\n",
       "      <td>16.175008</td>\n",
       "      <td>0.780382</td>\n",
       "      <td>0.487849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>47.432000</td>\n",
       "      <td>320.942611</td>\n",
       "      <td>0.352000</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>181.483754</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>0.738000</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.093092</td>\n",
       "      <td>176.850538</td>\n",
       "      <td>15666.690297</td>\n",
       "      <td>6.127421</td>\n",
       "      <td>307.699498</td>\n",
       "      <td>365.734414</td>\n",
       "      <td>12.065801</td>\n",
       "      <td>55.844536</td>\n",
       "      <td>3.439711</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.036752</td>\n",
       "      <td>196.967627</td>\n",
       "      <td>20927.833607</td>\n",
       "      <td>7.130299</td>\n",
       "      <td>333.073546</td>\n",
       "      <td>421.884968</td>\n",
       "      <td>14.218338</td>\n",
       "      <td>66.622485</td>\n",
       "      <td>3.955028</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.062066</td>\n",
       "      <td>216.667456</td>\n",
       "      <td>27332.762127</td>\n",
       "      <td>8.114887</td>\n",
       "      <td>359.950170</td>\n",
       "      <td>481.792304</td>\n",
       "      <td>16.557652</td>\n",
       "      <td>77.337473</td>\n",
       "      <td>4.500320</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>323.124000</td>\n",
       "      <td>61227.196008</td>\n",
       "      <td>13.127000</td>\n",
       "      <td>481.030642</td>\n",
       "      <td>753.342620</td>\n",
       "      <td>28.300000</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>6.739000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ph     Hardness        Solids  Chloramines      Sulfate  \\\n",
       "count  2785.000000  3276.000000   3276.000000  3276.000000  2495.000000   \n",
       "mean      7.080795   196.369496  22014.092526     7.122277   333.775777   \n",
       "std       1.594320    32.879761   8768.570828     1.583085    41.416840   \n",
       "min       0.000000    47.432000    320.942611     0.352000   129.000000   \n",
       "25%       6.093092   176.850538  15666.690297     6.127421   307.699498   \n",
       "50%       7.036752   196.967627  20927.833607     7.130299   333.073546   \n",
       "75%       8.062066   216.667456  27332.762127     8.114887   359.950170   \n",
       "max      14.000000   323.124000  61227.196008    13.127000   481.030642   \n",
       "\n",
       "       Conductivity  Organic_carbon  Trihalomethanes    Turbidity   Potability  \n",
       "count   3276.000000     3276.000000      3114.000000  3276.000000  3276.000000  \n",
       "mean     426.205111       14.284970        66.396293     3.966786     0.390110  \n",
       "std       80.824064        3.308162        16.175008     0.780382     0.487849  \n",
       "min      181.483754        2.200000         0.738000     1.450000     0.000000  \n",
       "25%      365.734414       12.065801        55.844536     3.439711     0.000000  \n",
       "50%      421.884968       14.218338        66.622485     3.955028     0.000000  \n",
       "75%      481.792304       16.557652        77.337473     4.500320     1.000000  \n",
       "max      753.342620       28.300000       124.000000     6.739000     1.000000  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#That gives us a sense of the distribution of the values.\n",
    "#We see that some ranges are very narrow and some are very wide.\n",
    "    #Hence we should use some normalization\n",
    "#From the above two we know that the data has 3276 rows and 10 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6138583638583639"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna().shape[0]/data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thus we know that simply dropping all of the rows that have a null in them might be problematic.\n",
    "#We'd be left with only abot 61% of our data.\n",
    "#So imputing values is a good idea, and we'll want a pipeline to test imputing methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first step in preprocessing the data is to split it up into training and testing sets.\n",
    "#We want three sets - training, validation, and testing\n",
    "#Ultimately the goal is 60% train, 20% validation, and 20% test\n",
    "#So we will start by splitting off the 60% of training data\n",
    "#And then we split the \"not-train\" set in half to generate the validation and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = train_test_split(data, test_size=.4, random_state=42)\n",
    "valid_set, test_set = train_test_split(val_set, test_size = .5, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to fit any models we';ll have to separate the features (predictors) fromt he values in each set.\n",
    "train_set_values = train_set.pop('Potability')\n",
    "valid_set_values = valid_set.pop('Potability')\n",
    "test_set_values = test_set.pop('Potability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And now we can run it through some machine learning classification algorithms to work out a relationship\n",
    "    #that will give good prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  13 tasks      | elapsed:    7.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'imputer__strategy': 'mean', 'regressor__C': 10000000000, 'regressor__penalty': 'l2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done  90 out of  90 | elapsed:    8.4s finished\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "#logistic pipe\n",
    "\n",
    "full_pipe_log = Pipeline([\n",
    "    ('imputer', SimpleImputer()), #there will, ultimately, be three options here.\n",
    "        #mean, median, and node\n",
    "    ('scaler', StandardScaler()), #this is statistical normalization (z - mu)/s.d.\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "        'imputer__strategy': [\"most_frequent\",\"mean\", \"median\"],\n",
    "        'classifier__C': [10**10,10**9,10**8],\n",
    "        'classifier__penalty': ['l1', 'l2']\n",
    "        }\n",
    "\n",
    "method_search = GridSearchCV(\n",
    "    full_pipe_log\n",
    "    , param_grid\n",
    "    , cv = 5\n",
    "    , scoring = 'neg_mean_squared_error'\n",
    "    , verbose = 4\n",
    "    , n_jobs = 6)\n",
    "method_search.fit(train_set, train_set_values)\n",
    "print(method_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[[419 236]]\n",
      "0.6396946564885496\n"
     ]
    }
   ],
   "source": [
    "#And then we can test its accuracy using the validation set\n",
    "pred_values = method_search.predict(valid_set)\n",
    "ct = np.array(pd.crosstab(pred_values, valid_set_values))\n",
    "print(pred_values)\n",
    "print(ct)\n",
    "print(ct.trace()/ct.sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So the logistic regression predicts everything is a zero (\"not potable\"), and that is correct in 63.97% of cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  13 tasks      | elapsed:    4.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'imputer__strategy': 'mean', 'regressor__C': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done  45 out of  45 | elapsed:    5.7s finished\n"
     ]
    }
   ],
   "source": [
    "#What about a Support Vector Machine (SVM) Classifier/\n",
    "full_pipe_SV = Pipeline([\n",
    "    ('imputer', SimpleImputer()), #there will, ultimately, be three options here.\n",
    "        #mean, median, and node\n",
    "    ('scaler', StandardScaler()), #this is statistical normalization (z - mu)/s.d.\n",
    "    ('classifier', SVC()) #will need to specify kernel type (linear, polynomial, RBF)\n",
    "])\n",
    "#Also specify regularization term C for how much error is OK\n",
    "#Also specify gamma for tightness of fit\n",
    "param_grid = {\n",
    "        'imputer__strategy': [\"most_frequent\",\"mean\", \"median\"],\n",
    "        'classifier__C': [10**1,10**0,10**-1]\n",
    "        }\n",
    "\n",
    "method_search = GridSearchCV(\n",
    "    full_pipe_SV\n",
    "    , param_grid\n",
    "    , cv = 5\n",
    "    , scoring = 'neg_mean_squared_error'\n",
    "    , verbose = 4\n",
    "    , n_jobs = 6)\n",
    "method_search.fit(train_set, train_set_values)\n",
    "print(method_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1\n",
      " 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1\n",
      " 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0\n",
      " 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0]\n",
      "[[384 164]\n",
      " [ 35  72]]\n",
      "0.6961832061068702\n"
     ]
    }
   ],
   "source": [
    "#And we can check the performance by corsstabuation again\n",
    "import numpy as np\n",
    "pred_values = method_search.predict(valid_set)\n",
    "ct = np.array(pd.crosstab(pred_values, valid_set_values))\n",
    "print(pred_values)\n",
    "print(ct)\n",
    "print(ct.trace()/ct.sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not everything is zero anymore.\n",
    "#Accuracy is still only 69.62% That's only about 5% better than the logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  14 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=6)]: Done 108 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=6)]: Done 600 tasks      | elapsed:   14.5s\n",
      "[Parallel(n_jobs=6)]: Done 1284 tasks      | elapsed:   24.0s\n",
      "[Parallel(n_jobs=6)]: Done 2168 tasks      | elapsed:   38.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'imputer__strategy': 'median', 'regressor__algorithm': 'ball_tree', 'regressor__leaf_size': 1, 'regressor__n_neighbors': 4, 'regressor__p': 2, 'regressor__weights': 'uniform'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done 2700 out of 2700 | elapsed:   50.1s finished\n"
     ]
    }
   ],
   "source": [
    "#What about K Nearest Neighbors? Maybe that would give better results?\n",
    "full_pipe_KNN = Pipeline([\n",
    "    ('imputer', SimpleImputer()), #there will, ultimately, be three options here.\n",
    "        #mean, median, and node\n",
    "    ('scaler', StandardScaler()), #this is statistical normalization (z - mu)/s.d.\n",
    "    ('classifier', KNeighborsClassifier())  #need to specify number of neighbors. Do this in the grid search\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "        'imputer__strategy': [\"most_frequent\",\"mean\", \"median\"],\n",
    "        'classifier__n_neighbors': [3,4,5],\n",
    "        'classifier__algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "        'classifier__weights':['uniform','distance'],\n",
    "        'classifier__leaf_size':[1,2,3,4,5],\n",
    "        'classifier__p':[1,2],\n",
    "        }\n",
    "\n",
    "method_search = GridSearchCV(\n",
    "    full_pipe_KNN\n",
    "    , param_grid\n",
    "    , cv = 5\n",
    "    , scoring = 'neg_mean_squared_error'\n",
    "    , verbose = 4\n",
    "    , n_jobs = 6)\n",
    "method_search.fit(train_set, train_set_values)\n",
    "print(method_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1\n",
      " 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0\n",
      " 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0\n",
      " 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1\n",
      " 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0\n",
      " 0 0 0 0 1 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      "[[373 175]\n",
      " [ 46  61]]\n",
      "0.6625954198473283\n"
     ]
    }
   ],
   "source": [
    "pred_values = method_search.predict(valid_set)\n",
    "ct = np.array(pd.crosstab(pred_values, valid_set_values))\n",
    "print(pred_values)\n",
    "print(ct)\n",
    "print(ct.trace()/ct.sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#That's worse than the Support Vector Classifier\n",
    "#66.26%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  13 tasks      | elapsed:   24.9s\n",
      "[Parallel(n_jobs=6)]: Done  86 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=6)]: Done 180 out of 180 | elapsed:  6.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'imputer__strategy': 'median', 'regressor__criterion': 'gini', 'regressor__min_samples_leaf': 1, 'regressor__n_estimators': 1200}\n"
     ]
    }
   ],
   "source": [
    "#Let's try a random forest classifier\n",
    "full_pipe_RF = Pipeline([\n",
    "    ('imputer', SimpleImputer()), #there will, ultimately, be three options here.\n",
    "        #mean, median, and node\n",
    "    ('scaler', StandardScaler()), #this is statistical normalization (z - mu)/s.d.\n",
    "    ('classifier', RandomForestClassifier())  \n",
    "])\n",
    "#max depth\n",
    "\n",
    "param_grid = {\n",
    "        'imputer__strategy': [\"most_frequent\",\"mean\", \"median\"],\n",
    "        'classifier__n_estimators': [800,1000,1200],\n",
    "        'classifier__min_samples_leaf':[1,2],\n",
    "        'classifier__criterion': ['gini', 'entropy']\n",
    "        }\n",
    "\n",
    "\n",
    "method_search = GridSearchCV(\n",
    "    full_pipe_RF\n",
    "    , param_grid\n",
    "    , cv = 5\n",
    "    , scoring = 'neg_mean_squared_error'\n",
    "    , verbose = 4\n",
    "    , n_jobs = 6)\n",
    "method_search.fit(train_set, train_set_values)\n",
    "print(method_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1\n",
      " 0 0 0 0 0 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0\n",
      " 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0\n",
      " 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      "[[375 157]\n",
      " [ 44  79]]\n",
      "0.6931297709923664\n"
     ]
    }
   ],
   "source": [
    "pred_values = method_search.predict(valid_set)\n",
    "ct = np.array(pd.crosstab(pred_values, valid_set_values))\n",
    "print(pred_values)\n",
    "print(ct)\n",
    "print(ct.trace()/ct.sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#69.31%\n",
    "#So random forest is the best I've see so far. Still less than 70 percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  13 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=6)]: Done  86 tasks      | elapsed:   29.3s\n",
      "[Parallel(n_jobs=6)]: Done 209 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=6)]: Done 380 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=6)]: Done 405 out of 405 | elapsed:  2.4min finished\n",
      "c:\\users\\t3\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:24:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "{'imputer__strategy': 'mean', 'regressor__learning_rate': 0.05, 'regressor__max_depth': 35, 'regressor__n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "#How about XGBoost?\n",
    "full_pipe_XGB = Pipeline([\n",
    "    ('imputer', SimpleImputer()), #there will, ultimately, be three options here.\n",
    "        #mean, median, and node\n",
    "    ('scaler', StandardScaler()), #this is statistical normalization (z - mu)/s.d.\n",
    "    ('classifier', XGBClassifier())  \n",
    "])\n",
    "#max depth\n",
    "\n",
    "param_grid = {\n",
    "        'imputer__strategy': [\"most_frequent\",\"mean\", \"median\"],\n",
    "        'classifier__learning_rate': [.07,.05,.03],\n",
    "        'classifier__max_depth': [30,35,40],\n",
    "        'classifier__n_estimators': [40,50,60]\n",
    "        }\n",
    "\n",
    "\n",
    "method_search = GridSearchCV(\n",
    "    full_pipe_XGB\n",
    "    , param_grid\n",
    "    , cv = 5\n",
    "    , scoring = 'neg_mean_squared_error'\n",
    "    , verbose = 4\n",
    "    , n_jobs = 6)\n",
    "method_search.fit(train_set, train_set_values)\n",
    "print(method_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1\n",
      " 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0\n",
      " 0 1 0 1 1 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
      " 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1\n",
      " 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0\n",
      " 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0\n",
      " 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 1 1 0\n",
      " 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 1 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
      " 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1\n",
      " 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0\n",
      " 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0]\n",
      "[[348 135]\n",
      " [ 71 101]]\n",
      "0.6854961832061068\n"
     ]
    }
   ],
   "source": [
    "pred_values = method_search.predict(valid_set)\n",
    "ct = np.array(pd.crosstab(pred_values, valid_set_values))\n",
    "print(pred_values)\n",
    "print(ct)\n",
    "print(ct.trace()/ct.sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#68.55%. No improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  14 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=6)]: Done 260 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=6)]: Done 752 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=6)]: Done 1178 tasks      | elapsed:   21.8s\n",
      "[Parallel(n_jobs=6)]: Done 1828 tasks      | elapsed:   32.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__learning_rate': 0.05, 'classifier__max_depth': 16, 'classifier__n_estimators': 50, 'classifier__num_leaves': 30, 'classifier__reg_alpha': 0.1, 'classifier__reg_lambda': 0.1, 'imputer__strategy': 'median'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done 2160 out of 2160 | elapsed:   39.6s finished\n"
     ]
    }
   ],
   "source": [
    "#Does Light GBM boost do any better?\n",
    "full_pipe_LGB = Pipeline([\n",
    "    ('imputer', SimpleImputer()), #there will, ultimately, be three options here.\n",
    "        #mean, median, and node\n",
    "    ('scaler', StandardScaler()), #this is statistical normalization (z - mu)/s.d.\n",
    "    ('classifier', LGBMClassifier())  \n",
    "])\n",
    "#max depth\n",
    "\n",
    "param_grid = {\n",
    "        'imputer__strategy': [\"most_frequent\",\"mean\", \"median\"],\n",
    "        'classifier__num_leaves': [10,30],\n",
    "        'classifier__max_depth': [16,32],\n",
    "        'classifier__learning_rate': [.01,.05],\n",
    "        'classifier__n_estimators': [20,50],\n",
    "        'classifier__reg_alpha': [0,.05,.1],\n",
    "        'classifier__reg_lambda': [0,.05,.1],\n",
    "        }\n",
    "\n",
    "\n",
    "method_search = GridSearchCV(\n",
    "    full_pipe_LGB\n",
    "    , param_grid\n",
    "    , cv = 5\n",
    "    , scoring = 'neg_mean_squared_error'\n",
    "    , verbose = 4\n",
    "    , n_jobs = 6)\n",
    "method_search.fit(train_set, train_set_values)\n",
    "print(method_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#{'classifier__learning_rate': 0.05, \n",
    "    #'classifier__max_depth': 16, \n",
    "    #'classifier__n_estimators': 50, \n",
    "    #'classifier__num_leaves': 30, \n",
    "    #'classifier__reg_alpha': 0.1, \n",
    "    #'classifier__reg_lambda': 0.1, \n",
    "    #'imputer__strategy': 'median'}\n",
    "#Even with the speed of LBGMClassifier this takes a while if the parameter grid is too large.\n",
    "#Alternatively that means that I can test more parameter sets.\n",
    "#But maybe I should just start using optuna all the time and give up on gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1\n",
      " 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 0\n",
      " 1 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 1 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0\n",
      " 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0\n",
      " 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0]\n",
      "[[367 153]\n",
      " [ 52  83]]\n",
      "0.6870229007633588\n"
     ]
    }
   ],
   "source": [
    "pred_values = method_search.predict(valid_set)\n",
    "ct = np.array(pd.crosstab(pred_values, valid_set_values))\n",
    "print(pred_values)\n",
    "print(ct)\n",
    "print(ct.trace()/ct.sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#68.70% Marginally better than XGBoost.\n",
    "#SVC is still best."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
